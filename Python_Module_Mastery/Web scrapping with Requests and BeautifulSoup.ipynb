{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Web Scraping Tutorial using BeautifulSoup\n",
    "   Part of it adapted from Vik Paruchuri\n",
    "   \n",
    "   When performing data science tasks, it’s common to want to use data found on the internet. You’ll usually be able to access this data in csv format, or via an Application Programming Interface(API). However, there are times when the data you want can only be accessed as part of a web page. In cases like this, you’ll want to use a technique called web scraping to get the data from the web page into a format you can work with in your analysis.\n",
    "   \n",
    "##   The components of a web page\n",
    "When we visit a web page, our web browser makes a request to a web server. This request is called a GET request, since we’re getting files from the server. The server then sends back files that tell our browser how to render the page for us. The files fall into a few main types:\n",
    "\n",
    "1. HTML – contain the main content of the page.\n",
    "2. CSS – add styling to make the page look nicer.\n",
    "3. JS – Javascript files add interactivity to web pages.\n",
    "4. Images – image formats, such as JPG and PNG allow web pages to show pictures.\n",
    "\n",
    "After our browser receives all the files, it renders the page and displays it to us. There’s a lot that happens behind the scenes to render a page nicely, but we don’t need to worry about most of it when we’re web scraping. When we perform web scraping, we’re interested in the main content of the web page, so we look at the HTML.\n",
    "\n",
    "## HTML\n",
    "HyperText Markup Language(HTML) is a language that web pages are created in. HTML isn’t a programming language, like Python – instead, it’s a markup language that tells a browser how to layout content. HTML allows you to do similar things to what you do in a word processor like Microsoft Word – make text bold, create paragraphs, and so on. Because HTML isn’t a programming language, it isn’t nearly as complex as Python.\n",
    "\n",
    "Let’s take a quick tour through HTML so we know enough to scrape effectively. HTML consists of elements called tags. The most basic tag is the <html> tag. This tag tells the web browser that everything inside of it is HTML. We can make a simple HTML document just using this tag:\n",
    "\n",
    "> `<html>`\n",
    "\n",
    "> `</html>`\n",
    "\n",
    "We haven’t added any content to our page yet, so if we viewed our HTML document in a web browser, we wouldn’t see anything:\n",
    "\n",
    "Right inside an html tag, we put two other tags, the head tag, and the body tag. The main content of the web page goes into the body tag. The head tag contains data about the title of the page, and other information that generally isn’t useful in web scraping:\n",
    "\n",
    "> `<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "    </body>\n",
    "> </html>`\n",
    ">\n",
    "\n",
    "We still haven’t added any content to our page (that goes inside the body tag), so we again won’t see anything:\n",
    "\n",
    "You may have noticed above that we put the head and body tags inside the html tag. In HTML, tags are nested, and can go inside other tags.\n",
    "\n",
    "We’ll now add our first content to the page, in the form of the p tag. The p tag defines a paragraph, and any text inside the tag is shown as a separate paragraph:\n",
    "\n",
    ">`<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p>\n",
    "            Here's a paragraph of text!\n",
    "        </p>\n",
    "        <p>\n",
    "            Here's a second paragraph of text!\n",
    "        </p>\n",
    "    </body>\n",
    "></html>`\n",
    "\n",
    "Here’s how this will look:\n",
    "\n",
    ">Here's a paragraph of text!\n",
    "\n",
    ">Here's a second paragraph of text!\n",
    "\n",
    "Tags have commonly used names that depend on their position in relation to other tags:\n",
    "\n",
    "+ child – a child is a tag inside another tag. So the two p tags above are both children of the body tag.\n",
    "+ parent – a parent is the tag another tag is inside. Above, the html tag is the parent of the body tag.\n",
    "+ sibiling – a sibiling is a tag that is nested inside the same parent as another tag. For example, head and body are siblings, since they’re both inside html. Both p tags are siblings, since they’re both inside body.\n",
    "\n",
    "We can also add properties to HTML tags that change their behavior:\n",
    "\n",
    ">`<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p>\n",
    "            Here's a paragraph of text!\n",
    "            <a href=\"https://www.dataquest.io\">Learn Data Science Online</a>\n",
    "        </p>\n",
    "        <p>\n",
    "            Here's a second paragraph of text!\n",
    "            <a href=\"https://www.python.org\">Python</a>\n",
    "        </p>\n",
    "    </body>\n",
    "></html>`\n",
    "\n",
    "Here’s how this will look:\n",
    "\n",
    ">Here's a paragraph of text! [Learn Data Science Online](https://www.dataquest.io)\n",
    "\n",
    ">Here's a second paragraph of text! [Python](https://www.python.org)\n",
    "\n",
    "In the above example, we added two a tags.  a tags are links, and tell the browser to render a link to another web page. The href property of the tag determines where the link goes.\n",
    "\n",
    "a and p are extremely common html tags. Here are a few others:\n",
    "\n",
    "+ div – indicates a division, or area, of the page.\n",
    "+ b – bolds any text inside.\n",
    "+ i – italicizes any text inside.\n",
    "+ table – creates a table.\n",
    "+ form – creates an input form.\n",
    "+ For a full list of tags, Google it, :-).\n",
    "\n",
    "Before we move into actual web scraping, let’s learn about the class and id properties. These special properties give HTML elements names, and make them easier to interact with when we’re scraping. One element can have multiple classes, and a class can be shared between elements. Each element can only have one id, and an id can only be used once on a page. Classes and ids are optional, and not all elements will have them.\n",
    "\n",
    "We can add classes and ids to our example:\n",
    "\n",
    ">`<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"bold-paragraph\">\n",
    "            Here's a paragraph of text!\n",
    "            <a href=\"https://www.dataquest.io\" id=\"learn-link\">Learn Data Science Online</a>\n",
    "        </p>\n",
    "        <p class=\"bold-paragraph extra-large\">\n",
    "            Here's a second paragraph of text!\n",
    "            <a href=\"https://www.python.org\" class=\"extra-large\">Python</a>\n",
    "        </p>\n",
    "    </body>\n",
    "></html>`\n",
    "\n",
    "Here’s how this will look:\n",
    "\n",
    ">Here's a paragraph of text! [Learn Data Science Online](https://www.dataquest.io)\n",
    "\n",
    ">Here's a second paragraph of text! [Python](https://www.python.org)\n",
    "\n",
    "As you can see, adding classes and ids doesn’t change how the tags are rendered at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The requests library\n",
    "\n",
    "The first thing we’ll need to do to scrape a web page is to download the page. We can download pages using the Python requests library. The requests library will make a GET request to a web server, which will download the HTML contents of a given web page for us. There are several different types of requests we can make using requests, of which GET is just one. If you want to learn more, check out [request documentation](http://docs.python-requests.org/en/master/).\n",
    "\n",
    "**Twitter, Spotify, Microsoft, Amazon, Lyft, BuzzFeed, Reddit, The NSA, Her Majesty's Government, Google, Twilio, Runscope, Mozilla, Heroku, PayPal, NPR, Obama for America, Transifex, Native Instruments, The Washington Post, SoundCloud, Kippt, Sony, and Federal U.S. Institutions that prefer to be unnamed claim to use Requests internally.**\n",
    "\n",
    "Let’s try downloading a simple sample website, http://dataquestio.github.io/web-scraping-pages/simple.html. We’ll need to first download it using the requests.get method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\") #the url of the page you want to download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running our request, we get a **Response object**. This object has a status_code property, which indicates if the page was downloaded successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A status_code of 200 means that the page downloaded successfully. We won’t fully dive into status codes here, but a status code starting with a 2 generally indicates success, and a code starting with a 4 or a 5 indicates an error.\n",
    "\n",
    "We can print out the HTML content of the page using the content property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content #get content of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.headers #get headers of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.encoding #get encoding of the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Parameters In URLs\n",
    "\n",
    "You often want to send some sort of data in the URL’s query string. If you were constructing the URL by hand, this data would be given as key/value pairs in the URL after a question mark, e.g. http://httpbin.org/get?key=val. Requests allows you to provide these arguments as a dictionary of strings, using the params keyword argument. As an example, if you wanted to pass key1=value1 and key2=value2 to http://httpbin.org/get, you would use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.get('http://httpbin.org/get', params=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the URL has been correctly encoded by printing the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://apple.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Headers\n",
    "\n",
    "If you’d like to add HTTP headers to a request, simply pass in a dict to the headers parameter.\n",
    "\n",
    "For example, we didn’t specify our user-agent in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://api.github.com/some/endpoint'\n",
    "headers = {'user-agent': 'my-app/0.0.1'}\n",
    "\n",
    "r = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, websites do not allow applications (for example, by python) to access, you have to send the request as if it is sent by a web browser.\n",
    "\n",
    "The header is the tool to fake the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make your python request as if it were from firefox browser\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko)'}\n",
    "r = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.headers # display the full information of the header of the page received"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Types\n",
    "There several response types:\n",
    "+ Text web page\n",
    "+ Bytes contents, e.g., images, files\n",
    "+ Json contents\n",
    "+ Raw contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text response\n",
    "r = requests.get('https://api.github.com/events')\n",
    "r.text[:200]#display the first 200 characters of the content\n",
    "#with r.text, Requests will automatically decode content from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bytes reponse contents\n",
    "r = requests.get('http://higheredutah.org/wp-content/uploads/2016/03/USU-search-cmte.jpg')\n",
    "r.content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "i = Image.open(BytesIO(r.content))\n",
    "i.show()\n",
    "\n",
    "#Save bytes contents\n",
    "with open('usu.jpg', 'wb') as fd:\n",
    "    fd.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Json response content\n",
    "r = requests.get('https://api.github.com/events')\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw response\n",
    "In the rare case that you'd like to get the raw socket response from the server, you can access r.raw. If you want to do this, make sure you set stream=True in your initial request. Once you do, you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.github.com/events', stream=True)\n",
    "#r.raw\n",
    "r.raw.read(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, however, you should use a pattern like this to save what is being streamed to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='test'\n",
    "with open(filename, 'wb') as fd: # 'wb' is write as bytes\n",
    "    for chunk in r.iter_content(chunk_size=128):\n",
    "        fd.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Response.iter_content will handle a lot of what you would otherwise have to handle when using Response.raw directly. When streaming a download, the above is the preferred and recommended way to retrieve the content. Note that chunk_size can be freely adjusted to a number that may better fit your use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is especially useful if you are downloading large files of which you may use just a small part. It will save your download time and storage space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.sec.gov/Archives/edgar/data/7323/0000065984-14-000065.txt' #this file is more than 400 megabytes.\n",
    "r = requests.get(url, stream=True)\n",
    "filename='test.txt'\n",
    "with open(filename, 'w') as fd:\n",
    "    n=0 #see how many chunk we download in the end\n",
    "    cont='' #considering the '<TYPE>GRAPHIC' may be spread in two chunks, we chop the last 12 characters in previous chunk and add it to the next chunk\n",
    "    for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "        test=cont+chunk.decode('utf-8') #adding the previous chunk's last 12 character to next chunk and decode bytes into string\n",
    "        inde=test.find('<TYPE>GRAPHIC')#search for the string in a string, if returns -1, meaning not found, otherwise the index of first stance\n",
    "        #inde=test.find('</html>') \n",
    "        if inde!=-1: #if found, write to file and break, but only write the string ending at '<\\html>'\n",
    "            print('found it')\n",
    "            fd.write(chunk.decode('utf-8')[:inde-12])#offsetting the added 12 characters of last chunk\n",
    "            break\n",
    "        fd.write(chunk.decode('utf-8')) #if not found,write current chunk to file\n",
    "        cont=str(chunk.decode('utf-8')[-12:])#retain the last 12 characters for next chunk\n",
    "        n+=1\n",
    "print(n)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing a page with BeautifulSoup\n",
    "\n",
    "As you can see above, we now have downloaded an HTML document.\n",
    "\n",
    "We can use the BeautifulSoup library to parse this document, and extract the text from the p tag. We first have to import the library, and create an instance of the BeautifulSoup class to parse our document:\n",
    "\n",
    "For more information, visit [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nytimes=requests.get('https://www.nytimes.com/section/technology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nytimes.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(nytimes.content, 'html.parser') #we need to specify a parser to soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text()[15000:17000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic tagging information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a #get the first a tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a.name #get the tag name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a.string #get the string between the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a['class'] #get the 'class' attribute of tag a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a['href'] #get the 'href' attribute of tag a, which the hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title #get the first title tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.parent.name # get the parent name of title tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.img #get the first 'img' tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all instances of a tag at once\n",
    "\n",
    "If we want to extract a single tag, we can instead use the find_all method, which will find all the instances of a tag on a page.\n",
    "\n",
    "`soup.find_all('a')` create an iterator for all 'a' tags\n",
    "\n",
    "`soup.find_all('div')` create an iterator for all 'div' tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all hyperlinks of a page\n",
    "\n",
    "links=[a['href'] for a in soup.find_all('a')]#list comprehension, function as if in a for loop:\n",
    "                                             #for x in soup.find_all('a')\n",
    "                                             #    links.append(x['href'])\n",
    "'''\n",
    "same as\n",
    "links=[]\n",
    "for a in soup.find_all('a'):\n",
    "    links.append(a['href'])\n",
    "\n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=set(links) #convert list to set, removing duplicates\n",
    "print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=list(l)\n",
    "links[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headlines=[x.get_text() for x in soup.find_all('h2', class_='headline')]#Note: the use of class_, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=set(headlines) #removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What if we only need the latest news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latest_panel=soup.find('section', id='latest-panel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hl=latest_panel.li #find the first 'li' tag within latest_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.a['href'] #get link from hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.h2.get_text().replace('\\n','').strip() #get the h2 tag from hl and get the text between tags, removing '\\n' and trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.time #get the 'time' tag from hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.time['datetime'] #get the 'datetime' attribute of time tage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, get all li tags from latest_panel, with id start with 'story-id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hls=latest_panel.find_all('li', id=lambda x: x and x.startswith('story-id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the headline titles, hyperlink, and date time of the headlines\n",
    "headlines=[(x.h2.get_text().replace('\\n','').strip(),x.a['href'], x.time['datetime']) for x in hls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
